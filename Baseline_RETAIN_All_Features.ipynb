{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf41b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b743b",
   "metadata": {},
   "source": [
    "### Reading Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99f3bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# define data path\n",
    "DATA_PATH = \"E:/CS_Master_Degree_UIUC/CS598_DeepLearning_for_Health_Data/Project/paper290/MIMIC_Processed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "63bffda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pids = pickle.load(open(os.path.join(DATA_PATH,'MIMICIIIPROCESSED.pids'), 'rb'))\n",
    "\n",
    "labels = pickle.load(open(os.path.join(DATA_PATH,'MIMICIIIPROCESSED.morts'), 'rb'))\n",
    "\n",
    "icd_seqs = pickle.load(open(os.path.join(DATA_PATH,'MIMICIIIPROCESSED.3digitICD9.seqs'), 'rb'))\n",
    "icd_types = pickle.load(open(os.path.join(DATA_PATH,'MIMICIIIPROCESSED.3digitICD9.types'), 'rb'))\n",
    "icd_rtypes = dict(zip(icd_types.values(), icd_types.keys()))\n",
    "\n",
    "med_seqs = pickle.load(open(os.path.join(DATA_PATH,'MIMICIIIPROCESSED.meds.seqs'), 'rb'))\n",
    "med_types = pickle.load(open(os.path.join(DATA_PATH,'MIMICIIIPROCESSED.meds.types'), 'rb'))\n",
    "med_rtypes = dict(zip(med_types.values(), med_types.keys()))\n",
    "\n",
    "lab_seqs = pickle.load(open(os.path.join(DATA_PATH,'MIMICIIIPROCESSED.abnlabs.seqs'), 'rb'))\n",
    "lab_types = pickle.load(open(os.path.join(DATA_PATH,'MIMICIIIPROCESSED.abnlabs.types'), 'rb'))\n",
    "lab_rtypes = dict(zip(lab_types.values(), lab_types.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "730b1b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of passed patients: 2825\n",
      "ratio of passed patients: 0.37\n"
     ]
    }
   ],
   "source": [
    "print(\"number of passed patients:\", sum(labels))\n",
    "print(\"ratio of passed patients: %.2f\" % (sum(labels) / len(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48855405",
   "metadata": {},
   "source": [
    "### Build concatenated seqs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "68a7f6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert integer seqs to string seqs\n",
    "\n",
    "def get_str_seqs(integer_seqs, rtypes):\n",
    "    seqs_str = []\n",
    "    for p in integer_seqs:\n",
    "        p_str = []\n",
    "        for v in p:\n",
    "            v_str = []\n",
    "            for i in v:\n",
    "                code = rtypes[i]\n",
    "                v_str.append(code)\n",
    "            p_str.append(v_str)\n",
    "        seqs_str.append(p_str)\n",
    "    return seqs_str\n",
    "    \n",
    "    \n",
    "icd_seqs_str = get_str_seqs(icd_seqs, icd_rtypes)\n",
    "med_seqs_str = get_str_seqs(med_seqs,med_rtypes)\n",
    "lab_seqs_str = get_str_seqs(lab_seqs, lab_rtypes)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "50e809c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all feature types and build the dictionary of all types\n",
    "types = list({**icd_types, **med_types, **lab_types}.keys())\n",
    "all_types = dict(zip(types, list(range(len(types)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "22a8a4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenated all features for each patient visit\n",
    "concat_seqs_str = []\n",
    "\n",
    "for i in range(len(pids)):\n",
    "    icd_seq = icd_seqs_str[i]\n",
    "    med_seq = med_seqs_str[i]\n",
    "    lab_seq = lab_seqs_str[i]\n",
    "    visits = []\n",
    "    for j in range(len(icd_seq)):\n",
    "        visit = icd_seq[j] + med_seq[j] + lab_seq[j]\n",
    "        visits.append(visit)\n",
    "    concat_seqs_str.append(visits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d8178d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the concatenated string seqs back to integer seqs\n",
    "\n",
    "def get_int_seqs(str_seqs, types):\n",
    "    seqs_int = []\n",
    "    for p in str_seqs:\n",
    "        p_int = []\n",
    "        for v in p:\n",
    "            v_int = []\n",
    "            for i in v:\n",
    "                code = types[i]\n",
    "                v_int.append(code)\n",
    "            p_int.append(v_int)\n",
    "        seqs_int.append(p_int)\n",
    "    return seqs_int\n",
    "\n",
    "concat_seqs = get_int_seqs(concat_seqs_str, all_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "04a3def0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7537\n",
      "[[0, 1, 2, 3, 4, 5, 6, 7, 942, 943, 944, 945, 942, 946, 947, 948, 949, 948, 950, 951, 949, 952, 953, 954, 955, 956, 957, 958, 959, 960, 960, 961, 962, 963, 963, 964, 965, 966, 947, 947, 947, 967, 967, 968, 969, 943, 970, 944, 971, 971, 972, 973, 950, 972, 972, 973, 949, 973, 969, 974, 975, 963, 960, 964, 976, 977, 957, 959, 978, 966, 961, 972, 979, 973, 969, 969, 972, 4144, 4145, 4146, 4144, 4145, 4146, 4144, 4145, 4147, 4146, 4144, 4145, 4147, 4148, 4146, 4149, 4150, 4151, 4152, 4153, 4154, 4155, 4146, 4150, 4148, 4146, 4156, 4157, 4158, 4150, 4151, 4153, 4154, 4159, 4144, 4146, 4158, 4158, 4150, 4151, 4153, 4154, 4159, 4150, 4150, 4150, 4160, 4159, 4161, 4162, 4144, 4145, 4148, 4146, 4157, 4150, 4154, 4155, 4160, 4144, 4148, 4146, 4147, 4148, 4146, 4150, 4144, 4147, 4148, 4146, 4147, 4148, 4146, 4147, 4148, 4146], [8, 9, 10, 1, 2, 11, 6, 7, 951, 951, 951, 980, 980, 980, 980, 981, 980, 982, 983, 966, 984, 985, 966, 986, 987, 981, 981, 981, 981, 981, 988, 989, 990, 991, 964, 980, 976, 976, 975, 992, 980, 993, 986, 970, 954, 954, 954, 954, 955, 955, 955, 955, 987, 982, 994, 980, 995, 969, 969, 969, 969, 969, 969, 980, 983, 951, 996, 975, 997, 981, 998, 999, 966, 960, 1000, 1001, 1002, 4158, 4150, 4151, 4163, 4164, 4159, 4158, 4150, 4151, 4159, 4161, 4158, 4165, 4150, 4151, 4153, 4159, 4166, 4162, 4144, 4167, 4146, 4168, 4162, 4144, 4145, 4167, 4146, 4169, 4156, 4158, 4150, 4151, 4152, 4153, 4159, 4161, 4158, 4170, 4150, 4151, 4153, 4159, 4161, 4158, 4171, 4150, 4151, 4152, 4153, 4159, 4161, 4172, 4158, 4165, 4171, 4150, 4151, 4153, 4154, 4159, 4158, 4165, 4171, 4150, 4151, 4152, 4153, 4159]]\n"
     ]
    }
   ],
   "source": [
    "print(len(concat_seqs))\n",
    "print(concat_seqs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc21c0e",
   "metadata": {},
   "source": [
    "### Build the Dataset\n",
    "\n",
    "1. CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0059c6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7537\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, input_seqs, labels):\n",
    "        self.x = input_seqs\n",
    "        self.y = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Return the number of samples (i.e. patients).\n",
    "        \"\"\"\n",
    "        return len(self.y)\n",
    "#         raise NotImplementedError\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \"\"\"\n",
    "        Generates one sample of data.\n",
    "        \"\"\"\n",
    "        \n",
    "        # your code here\n",
    "        return self.x[index], self.y[index]\n",
    "#         raise NotImplementedError\n",
    "        \n",
    "\n",
    "dataset = CustomDataset(concat_seqs, labels)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c870851",
   "metadata": {},
   "source": [
    "2. Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d9958b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "        sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "        is stored in `mask`.\n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
    "        masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of mortality labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "\n",
    "    sequences, labels = zip(*data)\n",
    "\n",
    "    y = torch.tensor(labels, dtype=torch.float)\n",
    "    \n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "\n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    rev_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        for j_visit, visit in enumerate(patient):\n",
    "            \"\"\"\n",
    "            update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "            \"\"\"\n",
    "            # your code here\n",
    "            num_code_pad = max_num_codes - len(visit)\n",
    "            x[i_patient][j_visit] = torch.tensor(visit + [0]*num_code_pad)\n",
    "            masks[i_patient][j_visit] = torch.tensor([1]*len(visit) + [0]*num_code_pad)\n",
    "        \n",
    "        num_true_visit = len(patient)\n",
    "        rev_x[i_patient][:num_true_visit] = torch.tensor(x[i_patient][:num_true_visit].tolist()[::-1])\n",
    "        rev_masks[i_patient][:num_true_visit] = torch.tensor(masks[i_patient][:num_true_visit].tolist()[::-1])\n",
    "\n",
    "    return x, masks, rev_x, rev_masks, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ef8a5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=10, collate_fn=collate_fn)\n",
    "loader_iter = iter(loader)\n",
    "x, masks, rev_x, rev_masks, y = next(loader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "98dc2550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 2107]) torch.Size([10, 5, 2107])\n",
      "torch.Size([10, 5, 2107]) torch.Size([10, 5, 2107])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, rev_x.shape)\n",
    "print(masks.shape, rev_masks.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67749279",
   "metadata": {},
   "source": [
    "3. Split dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a0056d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 6029\n",
      "Length of val dataset: 1508\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "train_size= int(len(dataset)*0.8)\n",
    "\n",
    "lengths = [train_size, len(dataset)-train_size]\n",
    "train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49a969d",
   "metadata": {},
   "source": [
    "4. Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8ae965d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, val_dataset, collate_fn):\n",
    "    \n",
    "    '''\n",
    "    Implement this function to return the data loader for  train, validation and test dataset. \n",
    "    Set batchsize to 32. Set `shuffle=True` only for train dataloader.\n",
    "    \n",
    "    Arguments:\n",
    "        train dataset: train dataset of type `CustomDataset`\n",
    "        val dataset: validation dataset of type `CustomDataset`\n",
    "        test dataset: test dataset of type `CustomDataset`\n",
    "        collate_fn: collate function\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, val_loader, test_loader: train, validation and test dataloaders\n",
    "    \n",
    "    Note that you need to pass the collate function to the data loader `collate_fn()`.\n",
    "    '''\n",
    "    \n",
    "    batch_size = 32\n",
    "   \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn = collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn = collate_fn)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4272ed6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189 48\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader), len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2926950",
   "metadata": {},
   "source": [
    "### RETAIN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56589d3",
   "metadata": {},
   "source": [
    "1. Alpha Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "83177923",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Define the linear layer `self.a_att` for alpha-attention using `nn.Linear()`;\n",
    "        \n",
    "        Arguments:\n",
    "            hidden_dim: the hidden dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        self.a_att = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, g):\n",
    "        \"\"\"\n",
    "        Implement the alpha attention.\n",
    "        \n",
    "        Arguments:\n",
    "            g: the output tensor from RNN-alpha of shape (batch_size, seq_length, hidden_dim) \n",
    "        \n",
    "        Outputs:\n",
    "            alpha: the corresponding attention weights of shape (batch_size, seq_length, 1)\n",
    "            \n",
    "        HINT: consider `torch.softmax`\n",
    "        \"\"\"\n",
    "        e = self.a_att(g)\n",
    "        alpha = torch.softmax(e, dim = 1)\n",
    "        return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdabefaf",
   "metadata": {},
   "source": [
    "2. beta attation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "afce494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Define the linear layer `self.b_att` for beta-attention using `nn.Linear()`;\n",
    "        \n",
    "        Arguments:\n",
    "            hidden_dim: the hidden dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        self.b_att = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self, h):\n",
    "        \"\"\"\n",
    "        Implement the beta attention.\n",
    "        \n",
    "        Arguments:\n",
    "            h: the output tensor from RNN-beta of shape (batch_size, seq_length, hidden_dim) \n",
    "        \n",
    "        Outputs:\n",
    "            beta: the corresponding attention weights of shape (batch_size, seq_length, hidden_dim)\n",
    "            \n",
    "        HINT: consider `torch.tanh`\n",
    "        \"\"\"\n",
    "        f = self.b_att(h)\n",
    "        beta = torch.tanh(f)\n",
    "        return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd09afba",
   "metadata": {},
   "source": [
    "3. Attention Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "be58ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_sum(alpha, beta, rev_v, rev_masks):\n",
    "    \"\"\"\n",
    "        mask select the hidden states for true visits (not padding visits) and then\n",
    "        sum the them up.\n",
    "\n",
    "    Arguments:\n",
    "        alpha: the alpha attention weights of shape (batch_size, seq_length, 1)\n",
    "        beta: the beta attention weights of shape (batch_size, seq_length, hidden_dim)\n",
    "        rev_v: the visit embeddings in reversed time of shape (batch_size, # visits, embedding_dim)\n",
    "        rev_masks: the padding masks in reversed time of shape (# visits, batch_size, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        c: the context vector of shape (batch_size, hidden_dim)\n",
    "        \n",
    "    NOTE: Do NOT use for loop.\n",
    "    \"\"\"\n",
    "    \n",
    "    b, v, e = rev_v.detach().numpy().shape\n",
    "    rev_masks_true = rev_masks.sum(dim=2)>0\n",
    "    rev_masks_true = rev_masks_true.reshape(b,v,1)\n",
    "    rev_v_padded = rev_v * rev_masks_true\n",
    "    c = torch.sum(alpha * beta * rev_v_padded, dim = 1)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fbeb3e",
   "metadata": {},
   "source": [
    "4. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9879cd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_embeddings_with_mask(x, masks):\n",
    "    \"\"\"\n",
    "    Mask select the embeddings for true visits (not padding visits) and then sum the embeddings for each visit up.\n",
    "\n",
    "    Arguments:\n",
    "        x: the embeddings of diagnosis sequence of shape (batch_size, # visits, # diagnosis codes, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        sum_embeddings: the sum of embeddings of shape (batch_size, # visits, embedding_dim)\n",
    "    \"\"\"\n",
    "    \n",
    "    x = x * masks.unsqueeze(-1)\n",
    "    x = torch.sum(x, dim = -2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ce6be154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RETAIN(\n",
       "  (embedding): Embedding(4428, 128)\n",
       "  (rnn_a): GRU(128, 128, batch_first=True)\n",
       "  (rnn_b): GRU(128, 128, batch_first=True)\n",
       "  (att_a): AlphaAttention(\n",
       "    (a_att): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (att_b): BetaAttention(\n",
       "    (b_att): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RETAIN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_codes, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        # Define the embedding layer using `nn.Embedding`. Set `embDimSize` to 128.\n",
    "        self.embedding = nn.Embedding(num_codes, embedding_dim)\n",
    "        # Define the RNN-alpha using `nn.GRU()`; Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "        self.rnn_a = nn.GRU(embedding_dim, embedding_dim, batch_first=True)\n",
    "        # Define the RNN-beta using `nn.GRU()`; Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "        self.rnn_b = nn.GRU(embedding_dim, embedding_dim, batch_first=True)\n",
    "        # Define the alpha-attention using `AlphaAttention()`;\n",
    "        self.att_a = AlphaAttention(embedding_dim)\n",
    "        # Define the beta-attention using `BetaAttention()`;\n",
    "        self.att_b = BetaAttention(embedding_dim)\n",
    "        # Define the linear layers using `nn.Linear()`;\n",
    "        self.fc = nn.Linear(embedding_dim, 1)\n",
    "        # Define the final activation layer using `nn.Sigmoid().\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, masks, rev_x, rev_masks):\n",
    "        '''\n",
    "        Arguments:\n",
    "            rev_x: the diagnosis sequence in reversed time of shape (# visits, batch_size, # diagnosis codes)\n",
    "            rev_masks: the padding masks in reversed time of shape (# visits, batch_size, # diagnosis codes)\n",
    "\n",
    "        Outputs:\n",
    "            probs: probabilities of shape (batch_size)\n",
    "        '''\n",
    "        # 1. Pass the reversed sequence through the embedding layer;\n",
    "        rev_x = self.embedding(rev_x)\n",
    "        # 2. Sum the reversed embeddings for each diagnosis code up for a visit of a patient.\n",
    "        rev_x = sum_embeddings_with_mask(rev_x, rev_masks)\n",
    "        # 3. Pass the reversed embegginds through the RNN-alpha and RNN-beta layer separately;\n",
    "        g, _ = self.rnn_a(rev_x)\n",
    "        h, _ = self.rnn_b(rev_x)\n",
    "        # 4. Obtain the alpha and beta attentions using `AlphaAttention()` and `BetaAttention()`;\n",
    "        alpha = self.att_a(g)\n",
    "        beta = self.att_b(h)\n",
    "        # 5. Sum the attention up using `attention_sum()`;\n",
    "        c = attention_sum(alpha, beta, rev_x, rev_masks)\n",
    "        # 6. Pass the context vector through the linear and activation layers.\n",
    "        logits = self.fc(c)\n",
    "        probs = self.sigmoid(logits)\n",
    "        return probs.squeeze()\n",
    "    \n",
    "\n",
    "# load the model here\n",
    "retain = RETAIN(num_codes = len(types))\n",
    "retain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e671c1",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5b5bd172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "\n",
    "def eval(model, val_loader):\n",
    "    \n",
    "    \"\"\"\n",
    "    Evaluate the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall precision score\n",
    "        recall: overall recall score\n",
    "        f1: overall f1 score\n",
    "        roc_auc: overall roc_auc score\n",
    "        \n",
    "    REFERENCE: checkout https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    model.eval()\n",
    "    for x, masks, rev_x, rev_masks, y in val_loader:\n",
    "        y_logit = model(x, masks, rev_x, rev_masks)\n",
    "        \"\"\"\n",
    "        obtain the predicted class (0, 1) by comparing y_logit against 0.5, \n",
    "        assign the predicted class to y_hat.\n",
    "        \"\"\"\n",
    "        y_hat = None\n",
    "        # your code here\n",
    "        y_hat = (y_logit > 0.5).int()\n",
    "#         raise NotImplementedError\n",
    "        y_score = torch.cat((y_score,  y_logit.detach().to('cpu')), dim=0)\n",
    "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "    \n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "    return p, r, f, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c6fbea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs):\n",
    "    \"\"\"\n",
    "    Train the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        train_loader: training dataloder\n",
    "        val_loader: validation dataloader\n",
    "        n_epochs: total number of epochs\n",
    "    \"\"\"\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, masks, rev_x, rev_masks, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x, masks, rev_x, rev_masks)\n",
    "            \"\"\" \n",
    "            TODO: calculate the loss using `criterion`, save the output to loss.\n",
    "            \"\"\"\n",
    "            loss = None\n",
    "            # your code here\n",
    "            loss = criterion(y_hat, y)\n",
    "#             raise NotImplementedError\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        p, r, f, roc_auc = eval(model, val_loader)\n",
    "        print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'.format(epoch+1, p, r, f, roc_auc))\n",
    "    return round(roc_auc, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "50137e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 0.856201\n",
      "Epoch: 1 \t Validation p: 0.59, r:0.45, f: 0.51, roc_auc: 0.70\n",
      "Epoch: 2 \t Training Loss: 0.665676\n",
      "Epoch: 2 \t Validation p: 0.56, r:0.64, f: 0.60, roc_auc: 0.74\n",
      "Epoch: 3 \t Training Loss: 0.655607\n",
      "Epoch: 3 \t Validation p: 0.59, r:0.55, f: 0.57, roc_auc: 0.76\n",
      "Epoch: 4 \t Training Loss: 0.555114\n",
      "Epoch: 4 \t Validation p: 0.59, r:0.69, f: 0.64, roc_auc: 0.77\n",
      "Epoch: 5 \t Training Loss: 0.517672\n",
      "Epoch: 5 \t Validation p: 0.59, r:0.64, f: 0.61, roc_auc: 0.76\n",
      "Epoch: 6 \t Training Loss: 0.510029\n",
      "Epoch: 6 \t Validation p: 0.59, r:0.73, f: 0.65, roc_auc: 0.78\n",
      "Epoch: 7 \t Training Loss: 0.472722\n",
      "Epoch: 7 \t Validation p: 0.68, r:0.52, f: 0.59, roc_auc: 0.78\n",
      "Epoch: 8 \t Training Loss: 0.437018\n",
      "Epoch: 8 \t Validation p: 0.65, r:0.54, f: 0.59, roc_auc: 0.78\n",
      "Epoch: 9 \t Training Loss: 0.411682\n",
      "Epoch: 9 \t Validation p: 0.63, r:0.63, f: 0.63, roc_auc: 0.79\n",
      "Epoch: 10 \t Training Loss: 0.404295\n",
      "Epoch: 10 \t Validation p: 0.68, r:0.51, f: 0.59, roc_auc: 0.79\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.79"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model\n",
    "retain = RETAIN(num_codes = len(types))\n",
    "\n",
    "# load the loss function\n",
    "criterion = nn.BCELoss()\n",
    "# load the optimizer\n",
    "optimizer = torch.optim.Adam(retain.parameters(), lr=1e-3)\n",
    "\n",
    "n_epochs = 10\n",
    "train(retain, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d2a85f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
