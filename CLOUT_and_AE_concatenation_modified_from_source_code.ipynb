{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a6092ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import argparse\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, classification_report, roc_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3144468",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, epochs=5, batchsize=50, vocabsize=5, embsize=100):\n",
    "        super(RNN, self).__init__()\n",
    "        self.epochs = 5\n",
    "        self.batchsize = batchsize\n",
    "        self.vocabsize = vocabsize\n",
    "        self.embsize = embsize\n",
    "\n",
    "        self.emb_icd = nn.Linear(vocabsize_icd, embsize_icd)\n",
    "        self.emb_meds = nn.Linear(vocabsize_meds, embsize_meds)\n",
    "        self.emb_labs = nn.Linear(vocabsize_labs, embsize_labs)\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=embsize, hidden_size=embsize, num_layers=1)\n",
    "        self.out = nn.Linear(embsize, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_icd, input_med, input_lab, input_latent, hidden=None, force=True, steps=0):\n",
    "        if force or steps == 0: steps = len(input_icd)\n",
    "        outputs = Variable(torch.zeros(steps, 1, 1))\n",
    "\n",
    "        input_icd = self.emb_icd(input_icd)\n",
    "        input_med = self.emb_meds(input_med)\n",
    "        input_lab = self.emb_labs(input_lab)\n",
    "\n",
    "        inputs = F.relu(torch.cat((input_icd, input_med, input_lab, input_latent),1))\n",
    "\n",
    "        inputs = inputs.view(inputs.size()[0],1,inputs.size()[1])\n",
    "        outputs, hidden = self.rnn(inputs, hidden)\n",
    "        outputs = self.out(outputs)\n",
    "        return outputs.squeeze(), hidden\n",
    "\n",
    "    def predict(self, input_icd, input_med, input_lab, input_latent):\n",
    "        out, hid = self.forward(input_icd, input_med, input_lab, input_latent, None)\n",
    "        return self.sig(out[-1]).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "562d0e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'E:/CS_Master_Degree_UIUC/CS598_DeepLearning_for_Health_Data/Project/paper290/MIMIC_Processed/'\n",
    "AE_PATH = 'E:/CS_Master_Degree_UIUC/CS598_DeepLearning_for_Health_Data/Project/paper290/Output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "802011b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4428, 175])\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "vocabsize_icd = 942\n",
    "vocabsize_meds = 3202\n",
    "vocabsize_labs = 284 #all 681\n",
    "vocabsize = vocabsize_icd+vocabsize_meds+vocabsize_labs\n",
    "\n",
    "embsize_icd = 50\n",
    "embsize_meds = 75\n",
    "embsize_labs = 50\n",
    "embsize_latent = 175\n",
    "embsize = embsize_icd + embsize_labs + embsize_meds + embsize_latent\n",
    "\n",
    "input_seqs_icd = pickle.load(open(DATA_PATH + 'MIMICIIIPROCESSED.3digitICD9.seqs', 'rb'))\n",
    "input_seqs_meds = pickle.load(open(DATA_PATH + 'MIMICIIIPROCESSED.meds.seqs', 'rb'))\n",
    "input_seqs_labs = pickle.load(open(DATA_PATH + 'MIMICIIIPROCESSED.abnlabs.seqs', 'rb'))\n",
    "input_seqs_fullicd = pickle.load(open(DATA_PATH + 'MIMICIIIPROCESSED.seqs', 'rb'))\n",
    "\n",
    "input_seqs_latent = pickle.load(open(AE_PATH + 'AE_Embeddings.seqs', 'rb'))\n",
    "# latent_weights = pickle.load(open(args.emb_weights))\n",
    "AE_emb_weights = torch.tensor(np.load(AE_PATH + 'AE_embedding_weights.npy',allow_pickle=True))\n",
    "print(AE_emb_weights.size())\n",
    "\n",
    "labels = pickle.load(open(DATA_PATH + 'MIMICIIIPROCESSED.morts', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a4b33eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded..\n"
     ]
    }
   ],
   "source": [
    "print('Data loaded..')\n",
    "\n",
    "trainratio = 0.7\n",
    "validratio = 0.1\n",
    "testratio = 0.2\n",
    "\n",
    "trainlindex = int(len(input_seqs_icd)*trainratio)\n",
    "validlindex = int(len(input_seqs_icd)*(trainratio + validratio))\n",
    "\n",
    "# Some functions that will be used during training for input seq conversion to one hot vector and the correlation calculation\n",
    "def convert_to_one_hot(code_seqs, len_):\n",
    "    new_code_seqs = []\n",
    "    for code_seq in code_seqs:\n",
    "        one_hot_vec = np.zeros(len_)\n",
    "        for code in code_seq:\n",
    "            one_hot_vec[code] = 1\n",
    "        new_code_seqs.append(one_hot_vec)\n",
    "    return np.array(new_code_seqs)\n",
    "\n",
    "def get_avg(seqs, type_):\n",
    "    count = 0\n",
    "    for seq in seqs:\n",
    "        count += len(seq)\n",
    "    val = round(count*1.0/len(seqs))\n",
    "    if type_ == 'i':\n",
    "        return min(4, int(val/5))\n",
    "    else:\n",
    "        return min(4, int(val/50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf38c4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training..\n",
      "Run 0\n",
      "Epoch 1\n",
      "0 tensor(55.7008)\n",
      "Validation AUC_ROC:  0.8721453440339902\n",
      "Test AUC_ROC:  0.8443293246304244\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.94      0.83       932\n",
      "           1       0.83      0.46      0.59       576\n",
      "\n",
      "    accuracy                           0.76      1508\n",
      "   macro avg       0.78      0.70      0.71      1508\n",
      "weighted avg       0.77      0.76      0.74      1508\n",
      "\n",
      "Run 1\n",
      "Epoch 1\n",
      "0 tensor(57.2297)\n",
      "Validation AUC_ROC:  0.8544506464157663\n",
      "Test AUC_ROC:  0.8391356783187226\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.88      0.81       894\n",
      "           1       0.76      0.56      0.65       614\n",
      "\n",
      "    accuracy                           0.75      1508\n",
      "   macro avg       0.75      0.72      0.73      1508\n",
      "weighted avg       0.75      0.75      0.74      1508\n",
      "\n",
      "Run 2\n",
      "Epoch 1\n",
      "0 tensor(54.1064)\n",
      "Validation AUC_ROC:  0.8741446778742101\n",
      "Test AUC_ROC:  0.8583998919628044\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.75      0.81       978\n",
      "           1       0.64      0.79      0.71       530\n",
      "\n",
      "    accuracy                           0.77      1508\n",
      "   macro avg       0.75      0.77      0.76      1508\n",
      "weighted avg       0.79      0.77      0.77      1508\n",
      "\n",
      "Run 3\n",
      "Epoch 1\n",
      "0 tensor(55.5761)\n",
      "Validation AUC_ROC:  0.8335525152081458\n",
      "Test AUC_ROC:  0.8601397686471898\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.80      0.82       937\n",
      "           1       0.70      0.78      0.73       571\n",
      "\n",
      "    accuracy                           0.79      1508\n",
      "   macro avg       0.78      0.79      0.78      1508\n",
      "weighted avg       0.79      0.79      0.79      1508\n",
      "\n",
      "Run 4\n",
      "Epoch 1\n",
      "0 tensor(52.3787)\n",
      "Validation AUC_ROC:  0.8369979004683571\n",
      "Test AUC_ROC:  0.8585012497898543\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.86      0.82       919\n",
      "           1       0.74      0.63      0.68       589\n",
      "\n",
      "    accuracy                           0.77      1508\n",
      "   macro avg       0.76      0.74      0.75      1508\n",
      "weighted avg       0.77      0.77      0.76      1508\n",
      "\n",
      "Run 5\n",
      "Epoch 1\n",
      "0 tensor(54.3919)\n",
      "Validation AUC_ROC:  0.8783569949719018\n",
      "Test AUC_ROC:  0.8669274149536111\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.92      0.85       956\n",
      "           1       0.81      0.59      0.68       552\n",
      "\n",
      "    accuracy                           0.80      1508\n",
      "   macro avg       0.80      0.75      0.77      1508\n",
      "weighted avg       0.80      0.80      0.79      1508\n",
      "\n",
      "Run 6\n",
      "Epoch 1\n",
      "0 tensor(54.8520)\n",
      "Validation AUC_ROC:  0.8647441010431105\n",
      "Test AUC_ROC:  0.8630773394555347\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.84      0.83       935\n",
      "           1       0.73      0.71      0.72       573\n",
      "\n",
      "    accuracy                           0.79      1508\n",
      "   macro avg       0.78      0.78      0.78      1508\n",
      "weighted avg       0.79      0.79      0.79      1508\n",
      "\n",
      "Run 7\n",
      "Epoch 1\n",
      "0 tensor(53.5676)\n",
      "Validation AUC_ROC:  0.8629075735755696\n",
      "Test AUC_ROC:  0.875991445768843\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.91      0.85       924\n",
      "           1       0.82      0.62      0.71       584\n",
      "\n",
      "    accuracy                           0.80      1508\n",
      "   macro avg       0.80      0.77      0.78      1508\n",
      "weighted avg       0.80      0.80      0.79      1508\n",
      "\n",
      "Run 8\n",
      "Epoch 1\n",
      "0 tensor(53.6733)\n",
      "Validation AUC_ROC:  0.8628688790401039\n",
      "Test AUC_ROC:  0.8664295441378947\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83       946\n",
      "           1       0.71      0.74      0.73       562\n",
      "\n",
      "    accuracy                           0.79      1508\n",
      "   macro avg       0.78      0.78      0.78      1508\n",
      "weighted avg       0.79      0.79      0.79      1508\n",
      "\n",
      "Run 9\n",
      "Epoch 1\n",
      "0 tensor(53.2640)\n",
      "Validation AUC_ROC:  0.8647529988885908\n",
      "Test AUC_ROC:  0.8528891920236445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.80      0.82       906\n",
      "           1       0.72      0.76      0.74       602\n",
      "\n",
      "    accuracy                           0.78      1508\n",
      "   macro avg       0.77      0.78      0.78      1508\n",
      "weighted avg       0.79      0.78      0.78      1508\n",
      "\n",
      "Average AUCROC: 0.8585820849688524 +/- 0.010347460318070505\n",
      "The training is complete!\n",
      "The time used is:  3567.703125\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.process_time()\n",
    "\n",
    "print('Starting training..')\n",
    "\n",
    "batchsize = 50\n",
    "\n",
    "# ICD_wise_tot_tr = np.zeros(5)\n",
    "# meds_wise_tot_tr = np.zeros(5)\n",
    "# labs_wise_tot_tr = np.zeros(5)\n",
    "\n",
    "# for i in range(len(train_input_seqs_icd)):\n",
    "# \tICD_wise_tot_tr[get_avg(train_input_seqs_icd[i], 'i')] += 1\n",
    "# \tmeds_wise_tot_tr[get_avg(train_input_seqs_meds[i], 'm')] += 1\n",
    "# \tlabs_wise_tot_tr[get_avg(train_input_seqs_labs[i], 'l')] += 1\n",
    "\n",
    "# print 'ICD-wise train total', ICD_wise_tot_tr\n",
    "# print 'Meds-wise train total', meds_wise_tot_tr\n",
    "# print 'Labs-wise train total', labs_wise_tot_tr\n",
    "\n",
    "best_aucrocs = []\n",
    "for run in range(10):\n",
    "    print('Run', run)\n",
    "\n",
    "    perm = np.random.permutation(len(input_seqs_icd))\n",
    "    rinput_seqs_icd = [input_seqs_icd[i] for i in perm]\n",
    "    rinput_seqs_meds = [input_seqs_meds[i] for i in perm]\n",
    "    rinput_seqs_labs = [input_seqs_labs[i] for i in perm]\n",
    "    rinput_seqs_latent = input_seqs_latent[perm]\n",
    "    rinput_seqs_fullicd = [input_seqs_fullicd[i] for i in perm]\n",
    "    rlabels = [labels[i] for i in perm]\n",
    "    rlabels = torch.tensor(rlabels)\n",
    "    \n",
    "    train_input_seqs_icd = rinput_seqs_icd[:trainlindex]\n",
    "    train_input_seqs_meds = rinput_seqs_meds[:trainlindex]\n",
    "    train_input_seqs_labs = rinput_seqs_labs[:trainlindex]\n",
    "    train_input_seqs_latent = rinput_seqs_latent[:trainlindex]\n",
    "    train_labels = rlabels[:trainlindex]\n",
    "    train_labels = train_labels.reshape(train_labels.shape[0],1)\n",
    "\n",
    "    valid_input_seqs_icd = rinput_seqs_icd[trainlindex:validlindex]\n",
    "    valid_input_seqs_meds = rinput_seqs_meds[trainlindex:validlindex]\n",
    "    valid_input_seqs_labs = rinput_seqs_labs[trainlindex:validlindex]\n",
    "    valid_input_seqs_latent = rinput_seqs_latent[trainlindex:validlindex]\n",
    "    valid_labels = rlabels[trainlindex:validlindex]\n",
    "\n",
    "    test_input_seqs_icd = rinput_seqs_icd[validlindex:]\n",
    "    test_input_seqs_meds = rinput_seqs_meds[validlindex:]\n",
    "    test_input_seqs_labs = rinput_seqs_labs[validlindex:]\n",
    "    test_input_seqs_latent = rinput_seqs_latent[validlindex:]\n",
    "    test_input_seqs_fullicd = rinput_seqs_fullicd[validlindex:]\n",
    "    test_labels = rlabels[validlindex:]\n",
    "\n",
    "    n_iters = len(train_input_seqs_icd)\n",
    "\n",
    "    model = RNN(n_epochs, 1, vocabsize, embsize)\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    aucrocs = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        print('Epoch', (epoch+1))\n",
    "\n",
    "        for i in (range(0, n_iters, batchsize)):\n",
    "            \n",
    "            batch_icd = train_input_seqs_icd[i:i+batchsize]\n",
    "            batch_meds = train_input_seqs_meds[i:i+batchsize]\n",
    "            batch_labs = train_input_seqs_labs[i:i+batchsize]\n",
    "            batch_latent = train_input_seqs_latent[i:i+batchsize]\n",
    "\n",
    "            batch_train_labels = train_labels[i:i+batchsize]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses = []\n",
    "\n",
    "            for j in range(len(batch_icd)):\n",
    "                icd_onehot = convert_to_one_hot(batch_icd[j], vocabsize_icd)\n",
    "                med_onehot = convert_to_one_hot(batch_meds[j], vocabsize_meds)\n",
    "                lab_onehot = convert_to_one_hot(batch_labs[j], vocabsize_labs)\n",
    "\n",
    "                icd_inputs = Variable(torch.from_numpy(icd_onehot).float())\n",
    "                med_inputs = Variable(torch.from_numpy(med_onehot).float())\n",
    "                lab_inputs = Variable(torch.from_numpy(lab_onehot).float())\n",
    "                \n",
    "                latent_inputs_oh = np.concatenate((icd_onehot, med_onehot, lab_onehot), 1)\n",
    "                latent_inputs = np.dot(latent_inputs_oh, AE_emb_weights)\n",
    "                latent_inputs = Variable(torch.from_numpy(latent_inputs).float())\n",
    "                # latent_inputs = Variable(batch_latent[j].float())\n",
    "\n",
    "\n",
    "                targets = Variable(batch_train_labels[j].float())\n",
    "\n",
    "                # Use teacher forcing 50% of the time\n",
    "                force = random.random() < 0.5\n",
    "                outputs, hidden = model(icd_inputs, med_inputs, lab_inputs, latent_inputs, None, force)\n",
    "                \n",
    "                #print outputs[-1], targets\n",
    "                loss = criterion(outputs[-1].view(1), targets)\n",
    "                losses.append(loss)\n",
    "            \n",
    "            loss = sum(losses)/len(batch_icd)\n",
    "#             print(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.data\n",
    "\n",
    "        print(epoch, epoch_loss)\n",
    "        \n",
    "        ## Validation phase\n",
    "        vpredictions = np.zeros(len(valid_input_seqs_icd))\n",
    "        for i in range(len(valid_input_seqs_icd)):\n",
    "            \n",
    "            icd_one_hot = convert_to_one_hot(valid_input_seqs_icd[i], vocabsize_icd)\n",
    "            meds_one_hot = convert_to_one_hot(valid_input_seqs_meds[i], vocabsize_meds)\n",
    "            labs_one_hot = convert_to_one_hot(valid_input_seqs_labs[i], vocabsize_labs)\n",
    "            \n",
    "            test_input_icd = Variable(torch.from_numpy(icd_one_hot).float())\n",
    "            test_input_med = Variable(torch.from_numpy(meds_one_hot).float())\n",
    "            test_input_lab = Variable(torch.from_numpy(labs_one_hot).float())\n",
    "\n",
    "            test_input_latent_oh = np.concatenate((icd_one_hot, meds_one_hot, labs_one_hot), 1)\n",
    "            test_input_latent = np.dot(test_input_latent_oh, AE_emb_weights)\n",
    "            test_input_latent = Variable(torch.from_numpy(test_input_latent).float())\n",
    "\n",
    "            # test_input_latent = Variable(torch.from_numpy(np.array(valid_input_seqs_latent[i])).float())\n",
    "            vpredictions[i] = model.predict(test_input_icd, test_input_med, test_input_lab, test_input_latent)\n",
    "\n",
    "        print(\"Validation AUC_ROC: \", roc_auc_score(valid_labels, vpredictions))\n",
    "        \n",
    "        ## Testing phase\n",
    "        predictions = np.zeros(len(test_input_seqs_icd))\n",
    "\n",
    "        ICD_wise_corr = np.zeros(5)\n",
    "        meds_wise_corr = np.zeros(5)\n",
    "        labs_wise_corr = np.zeros(5)\n",
    "        ICD_wise_tot = np.zeros(5)\n",
    "        meds_wise_tot = np.zeros(5)\n",
    "        labs_wise_tot = np.zeros(5)\n",
    "\n",
    "        for i in range(len(test_input_seqs_icd)):\n",
    "            icd_one_hot = convert_to_one_hot(test_input_seqs_icd[i], vocabsize_icd)\n",
    "            meds_one_hot = convert_to_one_hot(test_input_seqs_meds[i], vocabsize_meds)\n",
    "            labs_one_hot = convert_to_one_hot(test_input_seqs_labs[i], vocabsize_labs)\n",
    "            \n",
    "            test_input_icd = Variable(torch.from_numpy(icd_one_hot).float())\n",
    "            test_input_med = Variable(torch.from_numpy(meds_one_hot).float())\n",
    "            test_input_lab = Variable(torch.from_numpy(labs_one_hot).float())\n",
    "\n",
    "            test_input_latent_oh = np.concatenate((icd_one_hot, meds_one_hot, labs_one_hot), 1)\n",
    "            test_input_latent = np.dot(test_input_latent_oh, AE_emb_weights)\n",
    "            test_input_latent = Variable(torch.from_numpy(test_input_latent).float())\n",
    "\n",
    "            # test_input_latent = Variable(torch.from_numpy(np.array(test_input_seqs_latent[i])).float())\n",
    "            predictions[i] = model.predict(test_input_icd, test_input_med, test_input_lab, test_input_latent)\n",
    "\n",
    "            ICD_wise_corr[get_avg(test_input_seqs_icd[i], 'i')] += int((predictions[i]>0.5)*1 == test_labels[i])\n",
    "            ICD_wise_tot[get_avg(test_input_seqs_icd[i], 'i')] += 1\n",
    "\n",
    "            meds_wise_corr[get_avg(test_input_seqs_meds[i], 'm')] += int((predictions[i]>0.5)*1 == test_labels[i])\n",
    "            meds_wise_tot[get_avg(test_input_seqs_meds[i], 'm')] += 1\n",
    "\n",
    "            labs_wise_corr[get_avg(test_input_seqs_labs[i], 'l')] += int((predictions[i]>0.5)*1 == test_labels[i])\n",
    "            labs_wise_tot[get_avg(test_input_seqs_labs[i], 'l')] += 1\n",
    "\n",
    "        print(\"Test AUC_ROC: \", roc_auc_score(test_labels, predictions))\n",
    "\n",
    "        aucrocs.append(roc_auc_score(test_labels, predictions))\n",
    "        fpr, tpr, _ = roc_curve(test_labels, predictions)\n",
    "        pickle.dump({\"FPR\":fpr, \"TPR\":tpr}, open(AE_PATH+'roc_clout_cornn.p', 'wb'))\n",
    "        actual_predictions = (predictions>0.5)*1\n",
    "        print(classification_report(test_labels, actual_predictions))\n",
    "\n",
    "    best_aucrocs.append(max(aucrocs))\n",
    "\n",
    "print(\"Average AUCROC:\", np.mean(best_aucrocs), \"+/-\", np.std(best_aucrocs))\n",
    "\n",
    "\n",
    "end = time.process_time()\n",
    "print('The training is complete!')\n",
    "print('The time used is: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d649aaae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
