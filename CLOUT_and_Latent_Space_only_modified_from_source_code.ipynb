{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d07d705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import argparse\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, classification_report, roc_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f60f5b",
   "metadata": {},
   "source": [
    "### Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3566cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, epochs=5, batchsize=50, vocabsize=5, embsize=100):\n",
    "        super(RNN, self).__init__()\n",
    "        self.epochs = 5\n",
    "        self.batchsize = batchsize\n",
    "        self.vocabsize = vocabsize\n",
    "        self.embsize = embsize\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=embsize, hidden_size=embsize, num_layers=1)\n",
    "        self.out = nn.Linear(embsize, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_latent, hidden=None, force=True, steps=0):\n",
    "        if force or steps == 0: steps = len(input_latent)\n",
    "        outputs = Variable(torch.zeros(steps, 1, 1))\n",
    "        \n",
    "        inputs = F.relu(input_latent)\n",
    "\n",
    "        inputs = inputs.view(inputs.size()[0],1,inputs.size()[1])\n",
    "        outputs, hidden = self.rnn(inputs, hidden)\n",
    "        outputs = self.out(outputs)\n",
    "        return outputs.squeeze(), hidden\n",
    "\n",
    "    def predict(self, input_latent):\n",
    "        out, hid = self.forward(input_latent, None)\n",
    "        return self.sig(out[-1]).data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccb3b1d",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37d39b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'E:/CS_Master_Degree_UIUC/CS598_DeepLearning_for_Health_Data/Project/paper290/MIMIC_Processed/'\n",
    "CAE_PATH = 'E:/CS_Master_Degree_UIUC/CS598_DeepLearning_for_Health_Data/Project/paper290/Output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cb8b061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4428, 175])\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "vocabsize_icd = 942\n",
    "vocabsize_meds = 3202\n",
    "vocabsize_labs = 284 #all 681\n",
    "vocabsize = vocabsize_icd+vocabsize_meds+vocabsize_labs\n",
    "\n",
    "embsize_latent = 175\n",
    "embsize = embsize_latent\n",
    "\n",
    "input_seqs_icd = pickle.load(open(DATA_PATH + 'MIMICIIIPROCESSED.3digitICD9.seqs', 'rb'))\n",
    "input_seqs_meds = pickle.load(open(DATA_PATH + 'MIMICIIIPROCESSED.meds.seqs', 'rb'))\n",
    "input_seqs_labs = pickle.load(open(DATA_PATH + 'MIMICIIIPROCESSED.abnlabs.seqs', 'rb'))\n",
    "input_seqs_fullicd = pickle.load(open(DATA_PATH + 'MIMICIIIPROCESSED.seqs', 'rb'))\n",
    "\n",
    "input_seqs_latent = pickle.load(open(CAE_PATH + 'CAE_Embeddings.seqs', 'rb'))\n",
    "# latent_weights = pickle.load(open(args.emb_weights))\n",
    "CAE_emb_weights = torch.tensor(np.load(CAE_PATH + 'CAE_embedding_weights.npy',allow_pickle=True))\n",
    "print(CAE_emb_weights.size())\n",
    "\n",
    "labels = pickle.load(open(DATA_PATH + 'MIMICIIIPROCESSED.morts', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c08112e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded..\n"
     ]
    }
   ],
   "source": [
    "print('Data loaded..')\n",
    "\n",
    "trainratio = 0.7\n",
    "validratio = 0.1\n",
    "testratio = 0.2\n",
    "\n",
    "trainlindex = int(len(input_seqs_icd)*trainratio)\n",
    "validlindex = int(len(input_seqs_icd)*(trainratio + validratio))\n",
    "\n",
    "# Some functions that will be used during training for input seq conversion to one hot vector and the correlation calculation\n",
    "def convert_to_one_hot(code_seqs, len_):\n",
    "    new_code_seqs = []\n",
    "    for code_seq in code_seqs:\n",
    "        one_hot_vec = np.zeros(len_)\n",
    "        for code in code_seq:\n",
    "            one_hot_vec[code] = 1\n",
    "        new_code_seqs.append(one_hot_vec)\n",
    "    return np.array(new_code_seqs)\n",
    "\n",
    "def get_avg(seqs, type_):\n",
    "    count = 0\n",
    "    for seq in seqs:\n",
    "        count += len(seq)\n",
    "    val = round(count*1.0/len(seqs))\n",
    "    if type_ == 'i':\n",
    "        return min(4, int(val/5))\n",
    "    else:\n",
    "        return min(4, int(val/50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0faae4",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9097e28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training..\n",
      "Run 0\n",
      "Epoch 1\n",
      "0 tensor(61.3961)\n",
      "Validation AUC_ROC:  0.7469008264462811\n",
      "Test AUC_ROC:  0.755851886510443\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.85      0.78       927\n",
      "           1       0.65      0.47      0.54       581\n",
      "\n",
      "    accuracy                           0.70      1508\n",
      "   macro avg       0.69      0.66      0.66      1508\n",
      "weighted avg       0.69      0.70      0.69      1508\n",
      "\n",
      "Run 1\n",
      "Epoch 1\n",
      "0 tensor(61.6677)\n",
      "Validation AUC_ROC:  0.778673215016557\n",
      "Test AUC_ROC:  0.7576651599830776\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.87      0.78       926\n",
      "           1       0.67      0.43      0.53       582\n",
      "\n",
      "    accuracy                           0.70      1508\n",
      "   macro avg       0.69      0.65      0.65      1508\n",
      "weighted avg       0.69      0.70      0.68      1508\n",
      "\n",
      "Run 2\n",
      "Epoch 1\n",
      "0 tensor(62.0193)\n",
      "Validation AUC_ROC:  0.7480368079558546\n",
      "Test AUC_ROC:  0.7809256019405458\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.85      0.79       919\n",
      "           1       0.69      0.52      0.59       589\n",
      "\n",
      "    accuracy                           0.72      1508\n",
      "   macro avg       0.71      0.68      0.69      1508\n",
      "weighted avg       0.71      0.72      0.71      1508\n",
      "\n",
      "Run 3\n",
      "Epoch 1\n",
      "0 tensor(61.6840)\n",
      "Validation AUC_ROC:  0.7626736212858232\n",
      "Test AUC_ROC:  0.7611651063782258\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.88      0.79       931\n",
      "           1       0.68      0.42      0.52       577\n",
      "\n",
      "    accuracy                           0.70      1508\n",
      "   macro avg       0.70      0.65      0.65      1508\n",
      "weighted avg       0.70      0.70      0.68      1508\n",
      "\n",
      "Run 4\n",
      "Epoch 1\n",
      "0 tensor(61.6596)\n",
      "Validation AUC_ROC:  0.7420457614693059\n",
      "Test AUC_ROC:  0.7673358473824313\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.87      0.78       920\n",
      "           1       0.68      0.43      0.53       588\n",
      "\n",
      "    accuracy                           0.70      1508\n",
      "   macro avg       0.69      0.65      0.65      1508\n",
      "weighted avg       0.70      0.70      0.68      1508\n",
      "\n",
      "Run 5\n",
      "Epoch 1\n",
      "0 tensor(61.6648)\n",
      "Validation AUC_ROC:  0.7222393595810793\n",
      "Test AUC_ROC:  0.7594055755650717\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75       953\n",
      "           1       0.57      0.58      0.58       555\n",
      "\n",
      "    accuracy                           0.69      1508\n",
      "   macro avg       0.66      0.67      0.67      1508\n",
      "weighted avg       0.69      0.69      0.69      1508\n",
      "\n",
      "Run 6\n",
      "Epoch 1\n",
      "0 tensor(61.0181)\n",
      "Validation AUC_ROC:  0.7385093167701863\n",
      "Test AUC_ROC:  0.7504526634220438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.91      0.77       907\n",
      "           1       0.70      0.31      0.43       601\n",
      "\n",
      "    accuracy                           0.67      1508\n",
      "   macro avg       0.68      0.61      0.60      1508\n",
      "weighted avg       0.68      0.67      0.64      1508\n",
      "\n",
      "Run 7\n",
      "Epoch 1\n",
      "0 tensor(62.4401)\n",
      "Validation AUC_ROC:  0.7532234745662693\n",
      "Test AUC_ROC:  0.7457674050632912\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.78      0.75       948\n",
      "           1       0.58      0.51      0.54       560\n",
      "\n",
      "    accuracy                           0.68      1508\n",
      "   macro avg       0.65      0.64      0.65      1508\n",
      "weighted avg       0.67      0.68      0.67      1508\n",
      "\n",
      "Run 8\n",
      "Epoch 1\n",
      "0 tensor(61.7060)\n",
      "Validation AUC_ROC:  0.75508017355216\n",
      "Test AUC_ROC:  0.7446316083833746\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.87      0.76       912\n",
      "           1       0.66      0.39      0.49       596\n",
      "\n",
      "    accuracy                           0.68      1508\n",
      "   macro avg       0.67      0.63      0.63      1508\n",
      "weighted avg       0.67      0.68      0.66      1508\n",
      "\n",
      "Run 9\n",
      "Epoch 1\n",
      "0 tensor(61.8381)\n",
      "Validation AUC_ROC:  0.7712405116251271\n",
      "Test AUC_ROC:  0.7406009338427021\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.77      0.76       956\n",
      "           1       0.58      0.53      0.55       552\n",
      "\n",
      "    accuracy                           0.69      1508\n",
      "   macro avg       0.66      0.65      0.66      1508\n",
      "weighted avg       0.68      0.69      0.68      1508\n",
      "\n",
      "Average AUCROC: 0.7563801788471207 +/- 0.011363376176712875\n",
      "The training is complete!\n",
      "The time used is:  2076.796875\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.process_time()\n",
    "\n",
    "print('Starting training..')\n",
    "\n",
    "batchsize = 50\n",
    "\n",
    "# ICD_wise_tot_tr = np.zeros(5)\n",
    "# meds_wise_tot_tr = np.zeros(5)\n",
    "# labs_wise_tot_tr = np.zeros(5)\n",
    "\n",
    "# for i in range(len(train_input_seqs_icd)):\n",
    "# \tICD_wise_tot_tr[get_avg(train_input_seqs_icd[i], 'i')] += 1\n",
    "# \tmeds_wise_tot_tr[get_avg(train_input_seqs_meds[i], 'm')] += 1\n",
    "# \tlabs_wise_tot_tr[get_avg(train_input_seqs_labs[i], 'l')] += 1\n",
    "\n",
    "# print 'ICD-wise train total', ICD_wise_tot_tr\n",
    "# print 'Meds-wise train total', meds_wise_tot_tr\n",
    "# print 'Labs-wise train total', labs_wise_tot_tr\n",
    "\n",
    "best_aucrocs = []\n",
    "for run in range(10):\n",
    "    print('Run', run)\n",
    "\n",
    "    perm = np.random.permutation(len(input_seqs_icd))\n",
    "    rinput_seqs_icd = [input_seqs_icd[i] for i in perm]\n",
    "    rinput_seqs_meds = [input_seqs_meds[i] for i in perm]\n",
    "    rinput_seqs_labs = [input_seqs_labs[i] for i in perm]\n",
    "    rinput_seqs_latent = input_seqs_latent[perm]\n",
    "    rinput_seqs_fullicd = [input_seqs_fullicd[i] for i in perm]\n",
    "    rlabels = [labels[i] for i in perm]\n",
    "    rlabels = torch.tensor(rlabels)\n",
    "    \n",
    "    train_input_seqs_icd = rinput_seqs_icd[:trainlindex]\n",
    "    train_input_seqs_meds = rinput_seqs_meds[:trainlindex]\n",
    "    train_input_seqs_labs = rinput_seqs_labs[:trainlindex]\n",
    "    train_input_seqs_latent = rinput_seqs_latent[:trainlindex]\n",
    "    train_labels = rlabels[:trainlindex]\n",
    "    train_labels = train_labels.reshape(train_labels.shape[0],1)\n",
    "\n",
    "    valid_input_seqs_icd = rinput_seqs_icd[trainlindex:validlindex]\n",
    "    valid_input_seqs_meds = rinput_seqs_meds[trainlindex:validlindex]\n",
    "    valid_input_seqs_labs = rinput_seqs_labs[trainlindex:validlindex]\n",
    "    valid_input_seqs_latent = rinput_seqs_latent[trainlindex:validlindex]\n",
    "    valid_labels = rlabels[trainlindex:validlindex]\n",
    "\n",
    "    test_input_seqs_icd = rinput_seqs_icd[validlindex:]\n",
    "    test_input_seqs_meds = rinput_seqs_meds[validlindex:]\n",
    "    test_input_seqs_labs = rinput_seqs_labs[validlindex:]\n",
    "    test_input_seqs_latent = rinput_seqs_latent[validlindex:]\n",
    "    test_input_seqs_fullicd = rinput_seqs_fullicd[validlindex:]\n",
    "    test_labels = rlabels[validlindex:]\n",
    "\n",
    "    n_iters = len(train_input_seqs_icd)\n",
    "\n",
    "    model = RNN(n_epochs, 1, vocabsize, embsize)\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    aucrocs = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        print('Epoch', (epoch+1))\n",
    "\n",
    "        for i in (range(0, n_iters, batchsize)):\n",
    "            \n",
    "            batch_icd = train_input_seqs_icd[i:i+batchsize]\n",
    "            batch_meds = train_input_seqs_meds[i:i+batchsize]\n",
    "            batch_labs = train_input_seqs_labs[i:i+batchsize]\n",
    "            batch_latent = train_input_seqs_latent[i:i+batchsize]\n",
    "\n",
    "            batch_train_labels = train_labels[i:i+batchsize]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses = []\n",
    "\n",
    "            for j in range(len(batch_icd)):\n",
    "                icd_onehot = convert_to_one_hot(batch_icd[j], vocabsize_icd)\n",
    "                med_onehot = convert_to_one_hot(batch_meds[j], vocabsize_meds)\n",
    "                lab_onehot = convert_to_one_hot(batch_labs[j], vocabsize_labs)\n",
    "\n",
    "                \n",
    "                latent_inputs_oh = np.concatenate((icd_onehot, med_onehot, lab_onehot), 1)\n",
    "                latent_inputs = np.dot(latent_inputs_oh, CAE_emb_weights)\n",
    "                latent_inputs = Variable(torch.from_numpy(latent_inputs).float())\n",
    "                # latent_inputs = Variable(batch_latent[j].float())\n",
    "                \n",
    "\n",
    "                targets = Variable(batch_train_labels[j].float())\n",
    "\n",
    "                # Use teacher forcing 50% of the time\n",
    "                force = random.random() < 0.5\n",
    "                outputs, hidden = model(latent_inputs, None, force)\n",
    "                \n",
    "                #print outputs[-1], targets\n",
    "                loss = criterion(outputs[-1].view(1), targets)\n",
    "                losses.append(loss)\n",
    "            \n",
    "            loss = sum(losses)/len(batch_icd)\n",
    "#             print(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.data\n",
    "\n",
    "        print(epoch, epoch_loss)\n",
    "        \n",
    "        ## Validation phase\n",
    "        vpredictions = np.zeros(len(valid_input_seqs_icd))\n",
    "        for i in range(len(valid_input_seqs_icd)):\n",
    "            \n",
    "            icd_one_hot = convert_to_one_hot(valid_input_seqs_icd[i], vocabsize_icd)\n",
    "            meds_one_hot = convert_to_one_hot(valid_input_seqs_meds[i], vocabsize_meds)\n",
    "            labs_one_hot = convert_to_one_hot(valid_input_seqs_labs[i], vocabsize_labs)\n",
    "            \n",
    "\n",
    "            test_input_latent_oh = np.concatenate((icd_one_hot, meds_one_hot, labs_one_hot), 1)\n",
    "            test_input_latent = np.dot(test_input_latent_oh, CAE_emb_weights)\n",
    "            test_input_latent = Variable(torch.from_numpy(test_input_latent).float())\n",
    "\n",
    "            # test_input_latent = Variable(torch.from_numpy(np.array(valid_input_seqs_latent[i])).float())\n",
    "            vpredictions[i] = model.predict(test_input_latent)\n",
    "\n",
    "        print(\"Validation AUC_ROC: \", roc_auc_score(valid_labels, vpredictions))\n",
    "        \n",
    "        ## Testing phase\n",
    "        predictions = np.zeros(len(test_input_seqs_icd))\n",
    "\n",
    "        ICD_wise_corr = np.zeros(5)\n",
    "        meds_wise_corr = np.zeros(5)\n",
    "        labs_wise_corr = np.zeros(5)\n",
    "        ICD_wise_tot = np.zeros(5)\n",
    "        meds_wise_tot = np.zeros(5)\n",
    "        labs_wise_tot = np.zeros(5)\n",
    "\n",
    "        for i in range(len(test_input_seqs_icd)):\n",
    "            icd_one_hot = convert_to_one_hot(test_input_seqs_icd[i], vocabsize_icd)\n",
    "            meds_one_hot = convert_to_one_hot(test_input_seqs_meds[i], vocabsize_meds)\n",
    "            labs_one_hot = convert_to_one_hot(test_input_seqs_labs[i], vocabsize_labs)\n",
    "            \n",
    "            test_input_latent_oh = np.concatenate((icd_one_hot, meds_one_hot, labs_one_hot), 1)\n",
    "            test_input_latent = np.dot(test_input_latent_oh, CAE_emb_weights)\n",
    "            test_input_latent = Variable(torch.from_numpy(test_input_latent).float())\n",
    "\n",
    "            # test_input_latent = Variable(torch.from_numpy(np.array(test_input_seqs_latent[i])).float())\n",
    "            predictions[i] = model.predict(test_input_latent)\n",
    "\n",
    "            ICD_wise_corr[get_avg(test_input_seqs_icd[i], 'i')] += int((predictions[i]>0.5)*1 == test_labels[i])\n",
    "            ICD_wise_tot[get_avg(test_input_seqs_icd[i], 'i')] += 1\n",
    "\n",
    "            meds_wise_corr[get_avg(test_input_seqs_meds[i], 'm')] += int((predictions[i]>0.5)*1 == test_labels[i])\n",
    "            meds_wise_tot[get_avg(test_input_seqs_meds[i], 'm')] += 1\n",
    "\n",
    "            labs_wise_corr[get_avg(test_input_seqs_labs[i], 'l')] += int((predictions[i]>0.5)*1 == test_labels[i])\n",
    "            labs_wise_tot[get_avg(test_input_seqs_labs[i], 'l')] += 1\n",
    "\n",
    "        print(\"Test AUC_ROC: \", roc_auc_score(test_labels, predictions))\n",
    "\n",
    "        aucrocs.append(roc_auc_score(test_labels, predictions))\n",
    "        fpr, tpr, _ = roc_curve(test_labels, predictions)\n",
    "        pickle.dump({\"FPR\":fpr, \"TPR\":tpr}, open(CAE_PATH+'roc_clout_cornn.p', 'wb'))\n",
    "        actual_predictions = (predictions>0.5)*1\n",
    "        print(classification_report(test_labels, actual_predictions))\n",
    "\n",
    "    best_aucrocs.append(max(aucrocs))\n",
    "\n",
    "print(\"Average AUCROC:\", np.mean(best_aucrocs), \"+/-\", np.std(best_aucrocs))\n",
    "\n",
    "\n",
    "end = time.process_time()\n",
    "print('The training is complete!')\n",
    "print('The time used is: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0b7da7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
