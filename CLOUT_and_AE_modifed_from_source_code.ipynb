{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b23277de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import argparse\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06052414",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_path = 'E:/CS_Master_Degree_UIUC/CS598_DeepLearning_for_Health_Data/Project/paper290/Output/'\n",
    "data_path = 'E:/CS_Master_Degree_UIUC/CS598_DeepLearning_for_Health_Data/Project/paper290/MIMIC_Processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a235fc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4428, 175])\n"
     ]
    }
   ],
   "source": [
    "AE_emb_weights = torch.tensor(np.load(emb_path + 'AE_embedding_weights.npy',allow_pickle=True))\n",
    "print(AE_emb_weights.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6126bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "vocabsize = 175\n",
    "\n",
    "vocabsize_icd = 942\n",
    "vocabsize_meds = 3202\n",
    "vocabsize_labs = 284\n",
    "\n",
    "\n",
    "input_seqs_icd = pickle.load(open(data_path + 'MIMICIIIPROCESSED.3digitICD9.seqs', 'rb'))\n",
    "input_seqs_meds = pickle.load(open(data_path + 'MIMICIIIPROCESSED.meds.seqs', 'rb'))\n",
    "input_seqs_labs = pickle.load(open(data_path + 'MIMICIIIPROCESSED.abnlabs.seqs', 'rb'))\n",
    "\n",
    "labels = pickle.load(open(data_path +'MIMICIIIPROCESSED.morts', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99f2452",
   "metadata": {},
   "source": [
    "### Dataset built-up\n",
    "1. Convert all input_seqs to one-hot vectors [num_visits, batchsize=1, vocabsize of the seq]\n",
    "2. Concat all input_seqs of features by vocabsize dimension\n",
    "3. Multiply AE embedding weights to the concatenated input_seqs for each patient [num_visits, batchsize=1, vocabsize=175]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d593237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_seqs is a list of embeddings weighted by the AE_emb_weights. \n",
    "#The len of list = number of patients, each patient in the list is an numpy array [number of visits, emb_size=175]\n",
    "\n",
    "input_seqs = []\n",
    "\n",
    "def convert_to_one_hot(code_seqs, vocab):\n",
    "    new_code_seqs = torch.zeros((len(code_seqs),1,vocab))\n",
    "    for i, code_seq in enumerate(code_seqs):\n",
    "        for code in code_seq:\n",
    "            new_code_seqs[i][0][code] = 1\n",
    "    return new_code_seqs\n",
    "\n",
    "for i in range(len(input_seqs_icd)):\n",
    "    icd_onehot = convert_to_one_hot(input_seqs_icd[i], vocabsize_icd)\n",
    "    med_onehot = convert_to_one_hot(input_seqs_meds[i], vocabsize_meds)\n",
    "    lab_onehot = convert_to_one_hot(input_seqs_labs[i], vocabsize_labs)\n",
    "\n",
    "    latent_inputs_oh = torch.concat((icd_onehot, med_onehot, lab_onehot), 2)\n",
    "#     print(latent_inputs_oh.shape)\n",
    "    latent_inputs = torch.matmul(latent_inputs_oh, AE_emb_weights)\n",
    "    input_seqs.append(latent_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7ff7241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7537\n",
      "torch.Size([2, 1, 175])\n"
     ]
    }
   ],
   "source": [
    "print(len(input_seqs))\n",
    "print(input_seqs[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a987f5",
   "metadata": {},
   "source": [
    "### Define the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "985b6033",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, epochs, batchsize, vocabsize):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.epochs = 5\n",
    "        self.batchsize = batchsize\n",
    "        self.vocabsize = vocabsize\n",
    "        self.rnn = nn.LSTM(input_size=vocabsize, hidden_size=vocabsize, num_layers=1)\n",
    "        self.out = nn.Linear(vocabsize, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, hidden=None, force=True, steps=0):\n",
    "        if force or steps == 0: steps = len(x)\n",
    "        outputs = Variable(torch.zeros(steps, 1, 1))\n",
    "        x = F.relu(x)\n",
    "        x = x.view(x.size()[0],1,x.size()[1])\n",
    "        outputs, hidden = self.rnn(x, hidden)\n",
    "        outputs = self.out(outputs)\n",
    "        return outputs.squeeze(), hidden\n",
    "    \n",
    "    def predict(self, x):\n",
    "        out, hid = self.forward(x, None)\n",
    "        return self.sig(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab94bdfb",
   "metadata": {},
   "source": [
    "### Train the model and predict using validated and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f66fc3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainratio = 0.7\n",
    "validratio = 0.1\n",
    "testratio = 0.2\n",
    "\n",
    "trainlindex = int(len(input_seqs)*trainratio)\n",
    "validlindex = int(len(input_seqs)*(trainratio + validratio))\n",
    "batchsize = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1665db8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0\n",
      "Test AUC_ROC:  0.7037027855748109\n",
      "Test AUC_ROC:  0.7033233158408592\n",
      "Test AUC_ROC:  0.7030563522089334\n",
      "Test AUC_ROC:  0.7024728459848669\n",
      "Test AUC_ROC:  0.7003981571881864\n",
      "Run 1\n",
      "Test AUC_ROC:  0.6831080870361231\n",
      "Test AUC_ROC:  0.6817918316954287\n",
      "Test AUC_ROC:  0.6812900953390802\n",
      "Test AUC_ROC:  0.6792776464070358\n",
      "Test AUC_ROC:  0.6777128160159382\n",
      "Run 2\n",
      "Test AUC_ROC:  0.6865809977474983\n",
      "Test AUC_ROC:  0.6894501680144751\n",
      "Test AUC_ROC:  0.6910952328200584\n",
      "Test AUC_ROC:  0.6933865071452309\n",
      "Test AUC_ROC:  0.6939902514678188\n",
      "Run 3\n",
      "Test AUC_ROC:  0.6835964604675109\n",
      "Test AUC_ROC:  0.6845141723059934\n",
      "Test AUC_ROC:  0.6845365555215662\n",
      "Test AUC_ROC:  0.6884648098545837\n",
      "Test AUC_ROC:  0.6802669571510644\n",
      "Run 4\n",
      "Test AUC_ROC:  0.6730317864806867\n",
      "Test AUC_ROC:  0.6716030340963282\n",
      "Test AUC_ROC:  0.6680413984263233\n",
      "Test AUC_ROC:  0.6651559519551741\n",
      "Test AUC_ROC:  0.6647331008583691\n",
      "Run 5\n",
      "Test AUC_ROC:  0.6799725938181742\n",
      "Test AUC_ROC:  0.6839329376754062\n",
      "Test AUC_ROC:  0.6838802334795875\n",
      "Test AUC_ROC:  0.6854444187197775\n",
      "Test AUC_ROC:  0.6867940225912771\n",
      "Run 6\n",
      "Test AUC_ROC:  0.6748107456544438\n",
      "Test AUC_ROC:  0.6725343191450157\n",
      "Test AUC_ROC:  0.6738539171426816\n",
      "Test AUC_ROC:  0.6711485013205577\n",
      "Test AUC_ROC:  0.6749047970026412\n",
      "Run 7\n",
      "Test AUC_ROC:  0.6889436319722738\n",
      "Test AUC_ROC:  0.6898746499044888\n",
      "Test AUC_ROC:  0.6940304980656802\n",
      "Test AUC_ROC:  0.6938608716101213\n",
      "Test AUC_ROC:  0.6957595313683651\n",
      "Run 8\n",
      "Test AUC_ROC:  0.671601657474945\n",
      "Test AUC_ROC:  0.6731675858388187\n",
      "Test AUC_ROC:  0.6740015121864436\n",
      "Test AUC_ROC:  0.6748169068374547\n",
      "Test AUC_ROC:  0.6750281681788531\n",
      "Run 9\n",
      "Test AUC_ROC:  0.6887396321673278\n",
      "Test AUC_ROC:  0.6895698100733261\n",
      "Test AUC_ROC:  0.6898045888928958\n"
     ]
    }
   ],
   "source": [
    "best_aucrocs = []\n",
    "for run in range(10):\n",
    "    print('Run', run)\n",
    "    perm = np.random.permutation(len(input_seqs))\n",
    "    rinput_seqs = [input_seqs[i] for i in perm]\n",
    "    rlabels = [labels[i] for i in perm]\n",
    "\n",
    "    rlabels = torch.tensor(rlabels)\n",
    "    train_input_seqs = rinput_seqs[:trainlindex]\n",
    "    train_labels = rlabels[:trainlindex]\n",
    "    train_labels = train_labels.reshape(train_labels.shape[0],1)\n",
    "\n",
    "    valid_input_seqs = rinput_seqs[trainlindex:validlindex]\n",
    "    valid_labels = rlabels[trainlindex:validlindex]\n",
    "\n",
    "    test_input_seqs = rinput_seqs[validlindex:]\n",
    "    test_labels = rlabels[validlindex:]\n",
    "\n",
    "    n_iters = len(train_input_seqs)\n",
    "\n",
    "    model = MyRNN(n_epochs, 1, vocabsize)\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    aucrocs = []\n",
    "\n",
    "    n_epochs = 5\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        for i in (range(0, n_iters, batchsize)):\n",
    "            batch_icd = train_input_seqs[i:i+batchsize]\n",
    "            batch_train_labels = train_labels[i:i+batchsize]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses = []\n",
    "\n",
    "            for iter in range(len(batch_icd)):\n",
    "                icd_inputs = Variable(batch_icd[iter].float())\n",
    "                icd_inputs = icd_inputs.sum(axis=0)\n",
    "                targets = Variable(batch_train_labels[iter].float())\n",
    "\n",
    "                # Use teacher forcing 50% of the time\n",
    "                force = random.random() < 0.5\n",
    "                outputs, hidden = model(icd_inputs, None, force)\n",
    "                outputs = outputs.reshape((1))\n",
    "                #print outputs[-1], targets\n",
    "                losses.append(criterion(outputs, targets))\n",
    "\n",
    "            loss = sum(losses)/len(batch_icd)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        ## Validation phase\n",
    "        vpredictions = np.zeros(len(valid_input_seqs))\n",
    "        for i in range(len(valid_input_seqs)):\n",
    "            test_seq = valid_input_seqs[i]\n",
    "            icd_inputs = Variable(test_seq.float())\n",
    "            icd_inputs = icd_inputs.sum(axis=0)\n",
    "            vpredictions[i] = model.predict(icd_inputs)\n",
    "\n",
    "        ## Testing phase\n",
    "        predictions = np.zeros(len(test_input_seqs))\n",
    "        for i in range(len(test_input_seqs)):\n",
    "            test_seq = test_input_seqs[i]\n",
    "            icd_inputs = Variable(test_seq.float())\n",
    "            icd_inputs = icd_inputs.sum(axis=0)\n",
    "            predictions[i] = model.predict(icd_inputs)\n",
    "        print(\"Test AUC_ROC: \", roc_auc_score(test_labels, predictions))\n",
    "        actual_predictions = (predictions>0.5)*1\n",
    "        # print classification_report(test_labels, actual_predictions)\n",
    "\n",
    "        aucrocs.append(roc_auc_score(test_labels, predictions))\n",
    "best_aucrocs.append(max(aucrocs))\n",
    "\n",
    "print(\"Average AUCROC:\", np.mean(best_aucrocs), \"+/-\", np.std(best_aucrocs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092023c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
